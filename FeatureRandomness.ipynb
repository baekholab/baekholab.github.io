{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Impact of Feature Randomness in Random Forests***\n",
        "\n",
        "To illustrate the impact of feature randomoness in random forests, we'll build and compare two random forests. One that considers all features at each split, and another that considers a random subset of features at each split.\n",
        "\n"
      ],
      "metadata": {
        "id": "c5jmd3jF_Viv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 1: Import Necessary Libraries***"
      ],
      "metadata": {
        "id": "UCJDSWRyQCV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkbiKHss8c11"
      },
      "outputs": [],
      "source": [
        "# 1. Import Necessary Libraries:\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 2: Generate Synthetic Dataset***\n",
        "\n",
        "We'll create a synthetic dataset using the `make_classification` function with 1,000 samples (`n_samples`) and 20 features (`n_features`). We will set the number of informative features (`n_informative`) to 8 for now. `n_redundant = 0` means no features are replicates.\n",
        "\n",
        "`n_clusters_per_class` specifies the number of distinct clusters to generate within each class. Since 1 will create a dataset that is too simple, we'll set it to 2 in order to capture the effects of feature randomness."
      ],
      "metadata": {
        "id": "LoY89pJIQIrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Generate Synthetic Dataset:\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=20,\n",
        "                           n_informative=8, n_redundant=0,\n",
        "                           n_clusters_per_class=2, random_state=707)"
      ],
      "metadata": {
        "id": "gJh8J2HW8kDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 3: Splitting data***"
      ],
      "metadata": {
        "id": "IynnuPzefBN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Split the Dataset:\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
        "                                                    random_state=707)"
      ],
      "metadata": {
        "id": "8X0-lxgo81oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 4: Training a Random Forest without Feature Randomness***\n",
        "\n",
        "We'll make sure that this model considers all features at each split by keeping `max_features` at 20."
      ],
      "metadata": {
        "id": "T7_l6NcffHG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train Random Forest without Feature Randomness:\n",
        "\n",
        "rf_no_feature_randomness = RandomForestClassifier(n_estimators=200,\n",
        "                                                  max_features=20,\n",
        "                                                  random_state=707)\n",
        "rf_no_feature_randomness.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "xgfq6-_X88k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 5: Evaluating Model with `classification_report`***\n",
        "\n",
        "`classification_report` key metrics:\n",
        "- **Precision:** Indicates the accuracy of positive predictions for a class. It is calculated as the ratio of true positives to the sum of true positives and false positives.\n",
        "\n",
        "- **Recall (Sensitivity or True Positive Rate):** Measures the model's ability to identify all actual positive instances of a class. It is computed as the ratio of true positives to the sum of true positives and false negatives. High recall indicates that the model captures most of the actual positives for that class.\n",
        "\n",
        "- **F1-Score:** The harmonic mean of precision and recall, providing a single metric that balances both concerns. An F1-score close to 1 signifies a model with both high precision and recall for that class.\n",
        "\n",
        "- **Support:** Denotes the number of actual occurrences of each class in the dataset. It reflects how many instances of each class are present and is essential for understanding the context of the other metrics.\n",
        "\n",
        "- **Accuracy:** The overall proportion of correct predictions across all classes.​\n",
        "\n",
        "- **Macro Average:** The unweighted mean of the metrics for all classes, treating each class equally regardless of its support.​\n",
        "\n",
        "- **Weighted Average:** The mean of the metrics, weighted by the support of each class, accounting for class imbalance.\n",
        "\n"
      ],
      "metadata": {
        "id": "ezXPQWRCgQoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Evaluating Model with classification_report\n",
        "\n",
        "y_pred_no_fr = rf_no_feature_randomness.predict(X_test)\n",
        "accuracy_no_fr = accuracy_score(y_test, y_pred_no_fr)\n",
        "print(f'Accuracy without feature randomness: {accuracy_no_fr:.4f}')\n",
        "print('Classification Report without feature randomness:')\n",
        "print(classification_report(y_test, y_pred_no_fr))"
      ],
      "metadata": {
        "id": "GuFOafyI9AAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 6: Training a Random Forest with Feature Randomness***\n",
        "\n",
        "We'll limit `max_features` here to 4 (approx. sqrt(20)). Your task now is to experiment with different `max_features` values. Remember, the best `max_features` value is linked with the number of useful features."
      ],
      "metadata": {
        "id": "VKq7HaZmgr9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Train Random Forest with Feature Randomness:\n",
        "\n",
        "rf_with_feature_randomness = RandomForestClassifier(n_estimators=200,\n",
        "                                                    max_features=11,\n",
        "                                                    random_state=707)\n",
        "rf_with_feature_randomness.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "1GkR83Zw9C3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Step 7: Evaluating Model with `accuracy_score` and `classification_report`***\n",
        "\n",
        "Play around with the number of features considered at each split by controlling the `max_features` parameter in the code. Then, use the `classification_report` to see whether it helps the model."
      ],
      "metadata": {
        "id": "4xIxRpKmg3Co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate Model:\n",
        "y_pred_with_fr = rf_with_feature_randomness.predict(X_test)\n",
        "accuracy_with_fr = accuracy_score(y_test, y_pred_with_fr)\n",
        "print(f'Accuracy with feature randomness: {accuracy_with_fr:.4f}')\n",
        "print('Classification Report with feature randomness:')\n",
        "print(classification_report(y_test, y_pred_with_fr))"
      ],
      "metadata": {
        "id": "lImsMVtZ9Hac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Experiment***\n",
        "\n",
        "Experimentation suggestions:\n",
        "\n",
        "- Try experimenting with different `max_features` in Step 6. Evaluate changes with Step 7\n",
        "- Play around with `n_informative` to see how the number of informative features influence the best`max_features`.\n",
        "- Try increasing `n_clusters_per_class` see if the implementation of feature randomness improves model with more complex data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t09oRp7ClBPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***What did you find?***\n",
        "\n",
        "- You'll find that in general a reduced number of features considered at each split leads to a better performing model!\n",
        "\n",
        "- The best `max_feature` value hovers around the number of informative features.\n",
        "  - This could in turn mean that if all the features in a dataset are informative features, implementing feature randomness may be futile. However, with complex real life datasets, this is unlikely to be the case."
      ],
      "metadata": {
        "id": "qP4KK2Pzov8w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltIcZeJIqMBj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}